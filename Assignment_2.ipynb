{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYCVVv_1A1kZ"
      },
      "source": [
        "# Assignment 2: Bigram Language Model and Generative Pretrained Transformer (GPT)\n",
        "\n",
        "Due Date Feb 4th, 2024 11:59pm\n",
        "\n",
        "The objective of this assignment is to train a simplified transformer model. The primary differences between the implementation:\n",
        "* tokenizer (we use a character level encoder simplicity and compute constraints)\n",
        "* size (we are using 1 consumer grade gpu hosted on colab and a small dataset. in practice, the models are much larger and are trained on much more data)\n",
        "* efficiency\n",
        "\n",
        "\n",
        "Most modern LLMs have multiple training stages, so we won't get a model that is capable of replying to you yet. However, this is the first step towards a model like ChatGPT and Llama.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F097yaiu7dXQ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tD3r2BBNTjo"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCBAtO2ujwHK",
        "outputId": "4863dc13-44bd-4c5b-ae28-d549984e72c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qra06Ema_VL"
      },
      "source": [
        "## Part 1: Bigram MLP for TinyShakespeare (35 points)\n",
        "\n",
        "1a) (1 point). Create a list `chars` that contains all unique characters in `text`\n",
        "\n",
        "1b) (2 points). Implement `encode(s: str) -> list[int]`\n",
        "\n",
        "1c) (2 points). Implement `decode(ids: list[int]) -> str`\n",
        "\n",
        "1d) (5 points). Create two tensors, `inputs_one_hot` and `outputs_one_hot`. Use one hot encoding. Make sure to get every consecutive pair of characters. For example, for the word 'hello', we should create the following input-output pairs\n",
        "```\n",
        "he\n",
        "el\n",
        "ll\n",
        "lo\n",
        "```\n",
        "\n",
        "1e) (10 points). Implement BigramOneHotMLP, a 2 layer MLP that predicts the next token. Specifically, implement the constructor, forward, and generate. The output dimension of the first layer should be 8. Use `torch.optim`. The activation function for the first layer should be `nn.LeakyReLU()`\n",
        "\n",
        "Note: Use the `torch.nn.function.cross_entropy` loss. Read the [docs](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html) about how this loss function works. The logits are the output of a network WITHOUT an activation function applied to the last layer. There are activation functions are applied to every layer except the last.\n",
        "\n",
        "1f) (5 points). Train the BigramOneHotMLP for 1000 steps.\n",
        "\n",
        "1g) (5 points). Create two tensors, `input_ids` and `outputs_one_hot`. These `input_ids` will be used for the embedding layer.\n",
        "\n",
        "1h) (5 points). Implement and train BigramEmbeddingMLP, a 2 layer mlp that predicts the next token. Specifically, implement the constructor, forward, and generate functions. The output dimension of the first layer should be 8. Use `torch.optim`.\n",
        "\n",
        "\n",
        "\n",
        "Note: the output will look like gibberish\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iradmn7bZtM",
        "outputId": "19c36a86-cd15-47dc-ce9f-d2bedf257c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-05 00:09:58--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  6.98MB/s    in 0.2s    \n",
            "\n",
            "2024-02-05 00:09:58 (6.98 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLoVi294G-T-"
      },
      "outputs": [],
      "source": [
        "# For the bigram model, let's use the first 1000 characters for the data\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "text = text[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoMGZgEOdRjt"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Fw_c7h6KWwu"
      },
      "outputs": [],
      "source": [
        "#1a\n",
        "#append unique characters from text to chars list\n",
        "chars = sorted(set(text))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSuqxFb0KSi0"
      },
      "outputs": [],
      "source": [
        "#1b and 1c\n",
        "\n",
        "#dictionary to create character to id mapping\n",
        "chars2id = {c: i for i, c in enumerate(chars)}\n",
        "#dictionary to create id to character mapping\n",
        "id2chars = {i: c for c, i in chars2id.items()}\n",
        "\n",
        "#encode functions returns a list of all the ids corresponding to characters in string\n",
        "def encode(s: str) -> list[int]:\n",
        "    return [chars2id[c] for c in text]\n",
        "\n",
        "#decode function creates a string by getting all the characters corresponding to ids from the list returned by encode function\n",
        "def decode(ids: list[int]) -> str:\n",
        "    return ''.join([id2chars[i] for i in ids])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA2is3gWK5vy"
      },
      "outputs": [],
      "source": [
        "#1d\n",
        "#creates input and output tensors using one hot encoding which gives true values\n",
        "def create_one_hot_inputs_and_outputs() -> list[torch.tensor, torch.tensor]:\n",
        "    inputs_one_hot = []\n",
        "    outputs_one_hot = []\n",
        "    for c1, c2 in zip(text, text[1:]):\n",
        "        inputs_one_hot.append(F.one_hot(torch.tensor(chars2id[c1]), len(chars)))\n",
        "        outputs_one_hot.append(F.one_hot(torch.tensor(chars2id[c2]), len(chars)))\n",
        "    return torch.stack(inputs_one_hot), torch.stack(outputs_one_hot)\n",
        "\n",
        "inputs_one_hot, outputs_one_hot = create_one_hot_inputs_and_outputs()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrGw_vHpLO_o",
        "outputId": "e6d1cc33-557c-46b9-8583-12ce42fb8b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated word: akkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk\n"
          ]
        }
      ],
      "source": [
        "#1e\n",
        "#using bigram model to train to predict output i.e next character based on input which is the start character\n",
        "class BigramOneHotMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(len(chars), 8)\n",
        "        self.activation = nn.LeakyReLU()\n",
        "        self.fc2 = nn.Linear(8, len(chars))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def generate(self, start='a', max_new_tokens=100) -> str:\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            current_char = start\n",
        "            word = current_char\n",
        "            for _ in range(max_new_tokens):\n",
        "                input_ids = F.one_hot(torch.tensor(chars2id[current_char]), len(chars)).unsqueeze(0).float()\n",
        "                output = self(input_ids)\n",
        "                next_char_id = torch.argmax(output).item()\n",
        "                next_char = [k for k, v in chars2id.items() if v == next_char_id][0]\n",
        "                current_char = next_char\n",
        "                word += current_char\n",
        "            return word\n",
        "\n",
        "\n",
        "bigram_one_hot_mlp = BigramOneHotMLP()\n",
        "\n",
        "generated_word = bigram_one_hot_mlp.generate()\n",
        "print(f'Generated word: {generated_word}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_JHBQhCLdqv",
        "outputId": "0bcfd9bb-6910-4c13-fc0b-6ed0dba4b406"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a                                                                                                    \n",
            "101\n"
          ]
        }
      ],
      "source": [
        "#1f\n",
        "learning_rate = 0.01\n",
        "optimizer = optim.SGD(bigram_one_hot_mlp.parameters(), lr=learning_rate)\n",
        "\n",
        "#using this to understand the cross entropy loss between the true values and predictions\n",
        "# training loop\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    predictions = bigram_one_hot_mlp(inputs_one_hot.float())\n",
        "    loss = F.cross_entropy(predictions, outputs_one_hot.float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    #print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
        "    # if epoch%100==0:\n",
        "    #   print(f'Generated word:{bigram_one_hot_mlp.generate()}')\n",
        "print(bigram_one_hot_mlp.generate())\n",
        "print(len(bigram_one_hot_mlp.generate()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PasrfDz-dSqx"
      },
      "outputs": [],
      "source": [
        "#1g\n",
        "#using embedding instead of one hot encoding for input to give true values\n",
        "def create_embedding_inputs_and_outputs() -> list[torch.tensor, torch.tensor]:\n",
        "    inputs_ids=[]\n",
        "    outputs_one_hot=[]\n",
        "    for c1, c2 in zip(text, text[1:]):\n",
        "        inputs_ids.append(torch.tensor(chars2id[c1]))\n",
        "        outputs_one_hot.append(F.one_hot(torch.tensor(chars2id[c2]), len(chars)))\n",
        "    return torch.stack(inputs_ids), torch.stack(outputs_one_hot)\n",
        "\n",
        "\n",
        "input_ids, outputs_one_hot = create_embedding_inputs_and_outputs()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY3o2hFSL1DZ",
        "outputId": "c89ae14b-5fba-4444-ed68-2d4b4e99f427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated word: aCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;i\n",
            "Generated word: aCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;iCj;i\n"
          ]
        }
      ],
      "source": [
        "#1h\n",
        "#using bigram model to train to predict output i.e next character based on input which is the start character but here using embedding for input\n",
        "class BigramEmbeddingMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        # implement\n",
        "        super().__init__()\n",
        "        self.token_embedding=nn.Embedding(len(chars),8)\n",
        "        self.fc1 = nn.Linear(8, 8)\n",
        "        self.activation = nn.LeakyReLU()\n",
        "        self.fc2 = nn.Linear(8, len(chars))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # implement\n",
        "        x = self.token_embedding(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def generate(self, start='a', max_new_tokens=100) -> str:\n",
        "        # implement\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            current_char = start\n",
        "            word = current_char\n",
        "            for _ in range(max_new_tokens):\n",
        "                input_ids = torch.tensor(chars2id[current_char])\n",
        "                output = self(input_ids)\n",
        "                next_char_id = torch.argmax(output).item()\n",
        "                next_char = [k for k, v in chars2id.items() if v == next_char_id][0]\n",
        "                current_char = next_char\n",
        "                word += current_char\n",
        "            return word\n",
        "\n",
        "bigram_embedding_mlp = BigramEmbeddingMLP()\n",
        "\n",
        "generated_word = bigram_embedding_mlp.generate()\n",
        "print(f'Generated word: {generated_word}')\n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = optim.SGD(bigram_one_hot_mlp.parameters(), lr=learning_rate)\n",
        "\n",
        "#using this to understand the cross entropy loss between the true values and predictions\n",
        "# training loop\n",
        "for _ in range(1000):\n",
        "    # implement\n",
        "    optimizer.zero_grad()\n",
        "    predictions = bigram_one_hot_mlp(inputs_one_hot.float())\n",
        "    loss = F.cross_entropy(predictions, outputs_one_hot.float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "print(f'Generated word: {bigram_embedding_mlp.generate()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qplpM8_Cbp0s"
      },
      "source": [
        "## Part 2: Generative Pretrained Transformer (65 points)\n",
        "\n",
        "For this part, it is best to use a gpu. In the settings at the top go to Runtime -> Change Runtime Type and select T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Oh-3FeFxxnI",
        "outputId": "3356a19a-d07c-4116-f95b-64dfa83763c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb  5 00:10:49 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P0              30W /  70W |    173MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# run nvidia-smi to check gpu usage\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhJAwCAOADP7"
      },
      "outputs": [],
      "source": [
        "# For the gpt model, let's use the full text\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_LZpvZ8AEEi"
      },
      "source": [
        "Implement a character level tokenization function.\n",
        "\n",
        "1. Create a list of unique characters in the string. (1 points)\n",
        "2. Implement a function `encode(s: str) -> list[int]` that takes a string and returns a list of ids (1 point)\n",
        "3. Implement a function `decode(ids: list[int]) -> str` that takes a list of ids (ints) and returns a string (1 point)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnEOfMj4Dk4Y",
        "outputId": "ca0c8847-c83c-4cce-e1a9-5ec98cd1d16a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
          ]
        }
      ],
      "source": [
        "#1 Create a list of unique characters in the string.\n",
        "chars = sorted(set(text))\n",
        "print(len(chars))\n",
        "chars2id={c:i for i,c in enumerate(chars)}\n",
        "print(chars2id)\n",
        "id2chars={i:c for c,i in chars2id.items()}\n",
        "\n",
        "#2 Implement a function encode(s: str) -> list[int] that takes a string and returns a list of ids\n",
        "def encode(s: str) -> list[int]:\n",
        "    return [chars2id[c] for c in text]\n",
        "\n",
        "#Implement a function decode(ids: list[int]) -> str that takes a list of ids (ints) and returns a string\n",
        "def decode(ids: list[int]) -> str:\n",
        "    return ''.join([id2chars[i] for i in ids])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gyOaRF5Dq1P"
      },
      "outputs": [],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvWGi8Mk6x1q",
        "outputId": "938611b6-39d0-4d94-d273-63e007518581"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "block_size = 16\n",
        "data[:block_size+1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvO4hSK171Vu"
      },
      "source": [
        "To train a transformer, we feed the model `n` tokens (context) and try to predict the `n+1`th token (target) in the sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVWxO6Pa70Lh",
        "outputId": "65b77954-70b7-42a9-eb88-2e908e79ff48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47]) the target: 64\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64]) the target: 43\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43]) the target: 52\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52]) the target: 10\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10]) the target: 0\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0]) the target: 14\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14]) the target: 43\n"
          ]
        }
      ],
      "source": [
        "x = data[:block_size]\n",
        "y = data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFYZnm2MuLlt",
        "outputId": "0e2717bc-0920-4fbc-e1b8-acfbed646871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14])\n",
            "tensor([47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43])\n"
          ]
        }
      ],
      "source": [
        "#returns true values\n",
        "batch_size = 64\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def get_batch():\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x,y=x.to(device),y.to(device)\n",
        "    return x, y\n",
        "print(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HmnXJjxtm3N"
      },
      "source": [
        "### Single Self Attention Head (5 points)\n",
        "![](https://i.ibb.co/GWR1XG0/head.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SD8Z16R-sfZ",
        "outputId": "628d3bad-fc46-4b1b-eefa-32d3b176a5ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 32, 16])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define and initialize the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        #Linear projections for queries, keys, and values with head_size output dimensions\n",
        "        self.q_proj = nn.Linear(64, head_size, bias=False)\n",
        "        self.k_proj = nn.Linear(64, head_size, bias=False)\n",
        "        self.v_proj = nn.Linear(64, head_size, bias=False)\n",
        "        #dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "       # Project queries, keys, and values\n",
        "        B, T, C = x.shape\n",
        "        k = self.k_proj(x)  # (B,T,C)\n",
        "        q = self.q_proj(x)  # (B,T,C)\n",
        "        #perform matrix multiplication between query and key transpose\n",
        "        attention = q @ k.transpose(-2, -1) * C ** -0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "\n",
        "        mask = torch.tril(torch.ones(T, T)).to(device)\n",
        "        # Apply the mask to the attention scores\n",
        "        masked_attention = attention.masked_fill(mask == 0, float('-inf'))\n",
        "        # Apply softmax to get normalized attention weights\n",
        "        masked_attention = F.softmax(masked_attention, dim=-1)  # (B, T, T)\n",
        "        # Apply dropout to the attention weights\n",
        "        masked_attention = self.dropout(masked_attention)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.v_proj(x)  # (B,T,C)\n",
        "        #perform matrix multiplication between values and masked attention to get the output\n",
        "        out = masked_attention @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "# Instantiate SelfAttentionHead with a head size of 16 and move it to the device\n",
        "attn = SelfAttentionHead(16).to(device)\n",
        "x = torch.randn((8, 32, 64)).float().to(device) #input tensor with shape (batch_size=8, sequence_length=32, input_size=64) and move it to the device\n",
        "print(attn(x).shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWeoHGBiFpWd"
      },
      "source": [
        "### Multihead Self Attention (5 points)\n",
        "\n",
        "`constructor`\n",
        "\n",
        "- Create 4 `SelfAttentionHead` instances. Consider using `nn.ModuleList`\n",
        "- Create a linear layer with n_embd input dim and n_embd output dim\n",
        "\n",
        "`forward`\n",
        "\n",
        "In the forward implementation, pass `x` through each head, then concatenate all the outputs along the feature dimension, then pass the concatenated output through the linear layer\n",
        "\n",
        "![](https://i.ibb.co/y5SwyZZ/multihead.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFsPDkpnFs_b",
        "outputId": "36f5e43e-2268-4626-90ea-0896f641a260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 32, 64])\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n",
        "        self.linear = nn.Linear(num_heads*head_size,64)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        head_outputs = [head(x) for head in self.heads]\n",
        "        concatenated = torch.cat(head_outputs, dim=-1)\n",
        "\n",
        "        # Pass through linear layer\n",
        "        linear_output = self.linear(concatenated)\n",
        "        linear_output=self.dropout(linear_output)\n",
        "\n",
        "        return linear_output\n",
        "num_heads = 4 #initialising the number of heads to 4\n",
        "head_size = 16\n",
        "\n",
        "\n",
        "multi_head_attention = MultiHeadAttention(num_heads, head_size).to(device)\n",
        "\n",
        "# Example usage\n",
        "x = torch.randn((8, 32, 64)).float().to(device)\n",
        "output = multi_head_attention(x)\n",
        "print(output.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH_0ELyZ8YCC"
      },
      "source": [
        "## MLP (2 points)\n",
        "Implement a 2 layer MLP\n",
        "\n",
        "\n",
        "![](https://i.ibb.co/C0DtrF5/ff.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K96Z3kAv7lNt",
        "outputId": "957e04a1-e056-4ace-aebb-a0884e35e7a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 32, 64])\n"
          ]
        }
      ],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(64, 256)\n",
        "        self.activation = nn.LeakyReLU()\n",
        "        self.fc2 = nn.Linear(256, 64)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "        x=self.fc1(x)\n",
        "        x=self.activation(x)\n",
        "        x=self.fc2(x)\n",
        "        x=self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "x = torch.randn((8, 32, 64)).float()\n",
        "mlp_model=MLP()\n",
        "output = mlp_model(x)\n",
        "print(output.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUFxuyf-JIxr"
      },
      "source": [
        "## Transformer block (20 points)\n",
        "\n",
        "Layer normalization help training stability by normalizing the outputs of neurons within a single layer across all features for each individual data point, not across a full batch or a specific feature.\n",
        "\n",
        "Dropout is a form of regularization to prevent overfitting.\n",
        "\n",
        "This is the diagram of a transformer block:\n",
        "\n",
        "![](https://i.ibb.co/X85C473/block.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTDAd66KIvvx",
        "outputId": "c76c77cd-7f16-4722-c56a-beb203ce8916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 32, 64])\n"
          ]
        }
      ],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd: int, n_head: int):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(n_embd)\n",
        "        self.heads=MultiHeadAttention(n_head,16)\n",
        "        self.fc1 = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      y=self.norm1(x)\n",
        "      y=self.heads(y)\n",
        "      x=x+y\n",
        "      z=self.norm1(x)\n",
        "      z=self.fc1(z)\n",
        "      x=x+z\n",
        "      return x\n",
        "\n",
        "\n",
        "x = torch.randn((8, 32, 64)).float().to(device)\n",
        "tblock=Block(64,4).to(device)\n",
        "output=tblock(x)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyFQXltDKNti"
      },
      "source": [
        "## GPT\n",
        "\n",
        "`constructor` (5 points)\n",
        "\n",
        "1. create the token embedding table and the position embedding table\n",
        "2. create variable `self.blocks` that is a series of `Block`s. The data will pass through each block sequentially. Consider using `nn.Sequential`\n",
        "3. create a layer norm layer\n",
        "4. create a linear layer for predicting the next token\n",
        "\n",
        "`forward(self, idx, targets=None)`. (5 points)\n",
        "\n",
        "`forward` takes a batch of context ids as input of size (B, T) and returns the logits and the loss, if targets is not None. If targets is None, return the logits and None.\n",
        "1. get the token by using the token embedding table created in the constructor\n",
        "2. create the position embeddings\n",
        "3. sum the token and position embeddings to get the model input\n",
        "4. pass the model through the blocks, the layernorm layer, and the final linear layer\n",
        "5. compute the loss\n",
        "\n",
        "`generate(start_char, max_new_tokens, top_p, top_k, temperature) -> str` (5 points)\n",
        "1. implement top p, top_k, and temperature for sampling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xa2bh2XDdKy"
      },
      "source": [
        "![](https://i.ibb.co/n8sbQ0V/Screenshot-2024-01-23-at-8-59-08-PM.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WT4oUN084ts",
        "outputId": "499eb8e5-84b8-4638-f641-46ba97665d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "azvt Py$?pBGF Bjts-l&\n",
            "Yr,OpK!uXUCWS;v!kLCU!Jcz$q\n",
            "mWtKEMONUU?\n",
            "Px?KkHNr$$Fm!;BS-EwKrmtaKe-DhrMpjjjMU.&H\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        # create the token embedding table\n",
        "        self.token_embedding = nn.Embedding(len(chars),n_embd)\n",
        "\n",
        "        # Create the position embedding table\n",
        "        self.positional_embedding = nn.Embedding(32, n_embd)\n",
        "        # Create dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        # Create blocks using nn.Sequential to go through series of 4 blocks\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(4)])\n",
        "\n",
        "        # Create a layer norm layer\n",
        "        self.layer_norm = nn.LayerNorm(n_embd)\n",
        "\n",
        "        # Create a linear layer for predicting the next token\n",
        "        self.linear = nn.Linear(n_embd, len(chars))\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,T=idx.shape\n",
        "        # print(idx)\n",
        "        # print(B)\n",
        "        # print(T)\n",
        "        # print(len(chars))\n",
        "        # print(f\"Min index: {torch.min(idx)}, Max index: {torch.max(idx)}\")\n",
        "        # Create token emebeddings for the index passed from generate method\n",
        "        token_embeddings = self.token_embedding(idx).to(device)\n",
        "\n",
        "\n",
        "        # print(idx.size(1))\n",
        "\n",
        "        # Create position embeddings\n",
        "        positional_embedding = self.positional_embedding(torch.stack([torch.arange(T) for _ in range(B)], 0).to(device))\n",
        "        # print(token_embeddings.shape)\n",
        "        # print(positional_embedding.shape)\n",
        "\n",
        "        # Sum token and position embeddings\n",
        "        x = token_embeddings + positional_embedding\n",
        "        #pass the embedded output through dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # print(x.size())\n",
        "        # Pass through blocks\n",
        "        x= self.blocks(x)\n",
        "\n",
        "\n",
        "        # Apply layer norm\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # Linear layer for predicting the next token\n",
        "        logits = self.linear(x)\n",
        "\n",
        "        #return the loss and logits\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits,loss\n",
        "\n",
        "\n",
        "    def generate(self, start_char, max_new_tokens, top_p, top_k, temperature):\n",
        "        # Initialize the input sequence with the start character\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "          # id corresponding to the start character as tensor\n",
        "            current_seq = torch.tensor([chars2id[start_char]], dtype=torch.long).to(device).unsqueeze(0)\n",
        "            # print(current_seq)\n",
        "            for _ in range(max_new_tokens):\n",
        "                \"\"\"pass the last 32 elements from the tensor as 32 is the sequence length\n",
        "                for selecting a portion of the input sequence\"\"\"\n",
        "                logits, loss = self(current_seq[:, -32:])\n",
        "                # print(\"Logits shape:\", logits.shape)\n",
        "                # print(logits)\n",
        "\n",
        "                # Apply temperature to logits for diversity\n",
        "                logits = logits[:, -1, :]\n",
        "                # print(logits)\n",
        "                scaled_logits = logits / temperature\n",
        "                # Apply softmax to get probabilities\n",
        "                probabilities = F.softmax(scaled_logits, dim=-1)\n",
        "\n",
        "                # Apply top-k or top-p nucleus sampling\n",
        "                if top_k is not None:\n",
        "                    sampled_index = top_k_sampling(probabilities, k=top_k)\n",
        "                elif top_p is not None:\n",
        "                    sampled_index = top_p_sampling(probabilities, p=top_p)\n",
        "\n",
        "\n",
        "                # Convert the sampled index to a PyTorch tensor\n",
        "                sampled_token = torch.tensor([[sampled_index]]).to(device)\n",
        "                # print(sampled_token)\n",
        "\n",
        "                # Append the sampled token to the sequence\n",
        "                current_seq = torch.cat([current_seq, sampled_token], dim=1)\n",
        "\n",
        "        #print(\"Loss\",loss)\n",
        "        # Convert the generated sequence to a string using ids2character mapping\n",
        "        generated_string = ''.join([id2chars[token.item()] for token in current_seq[0]])\n",
        "        return generated_string\n",
        "\n",
        "def top_k_sampling(probabilities, k=5):\n",
        "        \"\"\"\n",
        "        Performs top-k sampling from a probability distribution.\n",
        "\n",
        "        :param probabilities: A 1D numpy array containing the probability distribution.\n",
        "        :param k: The number of top elements to consider for sampling.\n",
        "        :return: An index sampled from the top-k distribution.\n",
        "        \"\"\"\n",
        "        probabilities = probabilities.cpu().numpy().flatten()\n",
        "      # Get indices of the top k probabilities\n",
        "        top_k_indices = np.argsort(probabilities)[-k:]\n",
        "\n",
        "      # Extract the top k probabilities\n",
        "        top_k_probabilities = probabilities[top_k_indices]\n",
        "\n",
        "      # Normalize the top k probabilities so they sum to 1\n",
        "        top_k_probabilities /= top_k_probabilities.sum()\n",
        "\n",
        "      # Sample from the top k elements\n",
        "        chosen_index = np.random.choice(top_k_indices, p=top_k_probabilities)\n",
        "\n",
        "        return chosen_index\n",
        "\n",
        "def top_p_sampling(probabilities, p=0.9):\n",
        "    \"\"\"\n",
        "    Selects tokens from a probability distribution that have a cumulative probability\n",
        "    greater than the threshold p.\n",
        "\n",
        "    :param probabilities: A 1D numpy array containing the probability distribution.\n",
        "    :param p: The cumulative probability threshold (0 < p <= 1).\n",
        "    :return: An index sampled according to the top-p distribution.\n",
        "    \"\"\"\n",
        "    probabilities = probabilities.cpu().numpy().flatten()\n",
        "    # print(f\"{probabilities=}\")\n",
        "    # Sort probabilities in descending order and also get the original indices\n",
        "    if len(probabilities) > 1:\n",
        "      sorted_indices = np.argsort(probabilities)[::-1]\n",
        "    else:\n",
        "    # Handle the case where probabilities is empty or has length 1\n",
        "      sorted_indices = np.argsort(probabilities)\n",
        "    # print(f\"{sorted_indices=}\")\n",
        "\n",
        "    sorted_probabilities = probabilities[sorted_indices]\n",
        "\n",
        "    # print(f\"{sorted_probabilities=}\")\n",
        "\n",
        "    # Compute cumulative probabilities\n",
        "    cumulative_probabilities = np.cumsum(sorted_probabilities)\n",
        "    # print(f\"{cumulative_probabilities=}\")\n",
        "\n",
        "    # Find the threshold index\n",
        "    cutoff_index = np.where(cumulative_probabilities > p)[0][0]\n",
        "    # print(f\"{cutoff_index=}\")\n",
        "\n",
        "    # Filter out indices and probabilities that don't meet the threshold\n",
        "    filtered_indices = sorted_indices[:cutoff_index + 1]\n",
        "    # print(f\"{filtered_indices=}\")\n",
        "    filtered_probabilities = sorted_probabilities[:cutoff_index + 1]\n",
        "    # print(f\"{filtered_probabilities=}\")\n",
        "\n",
        "    # Normalize the filtered probabilities\n",
        "    filtered_probabilities /= filtered_probabilities.sum()\n",
        "    # print(f\"normalized_filtered_probabilities{filtered_probabilities}\")\n",
        "\n",
        "    # Sample from the filtered distribution\n",
        "    chosen_index = np.random.choice(filtered_indices, p=filtered_probabilities)\n",
        "\n",
        "    return chosen_index\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "gpt_model = GPT(64, 4).to(device)\n",
        "# Correcting the variable name for model in the generate method\n",
        "generated_text = gpt_model.generate(start_char='a', max_new_tokens=100,top_k=None,top_p=0.9, temperature=1.0)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njzrwwiv-mfB"
      },
      "source": [
        "### Training loop (15 points)\n",
        "\n",
        "implement training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWtn2uTwYUrY",
        "outputId": "cedc2c68-0e5e-4cd9-ecef-285d58289a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, Loss: 4.3076491355896\n",
            "Generated text:ayKDjbVmwg!&GllgBf'oXI'z'td:P.gWoFPa3,HxKHEHHHHHozz!NKHxKHEHxIoz!NxEiziHKHHHxIHHxKz!:PaaobHHEnFPcxKz,\n",
            "Iteration 1001, Loss: 3.5863616466522217\n",
            "Generated text:ar t t  he    t leaoehhy    cenhhh a  c a  aot  t aoeo a c  cai too a co cot eha aihenhhoh a ahhea   \n",
            "Iteration 2001, Loss: 3.2973084449768066\n",
            "Generated text:anith   sto e terest ee  tthe t t oheotooo  aoe oth t  eo thao  oth teo e  o ee  a ootheaaaao   eoo t\n",
            "Iteration 3001, Loss: 3.1949102878570557\n",
            "Generated text:anhe t t  toe t e thanthe stoeo  t the  a  othenit aaosa t o t te  a aen sea  oha toothheaaa a s to s\n",
            "Iteration 4001, Loss: 3.0600011348724365\n",
            "Generated text:at te t s t   se  t t e aenaee aos t t aoo  aa a aaa o a ao othaotis a o os arisa anot t sa osea the \n",
            "Iteration 5001, Loss: 3.0313384532928467\n",
            "Generated text:aroor t th eaouer tor seeareae t too  o an tenhe an a teonos  a i t t th the the a  t iaa sooshaa t s\n",
            "Iteration 6001, Loss: 3.0814859867095947\n",
            "Generated text:athe m t noere ad aou t is t\n",
            "u seao saa the teaora ior a anha te t ia t aar s isanhite se se haanaaot\n",
            "Iteration 7001, Loss: 3.060865879058838\n",
            "Generated text:ale t sand tt a aeares t nde t antha thor he than s h to s t the iin a h he th inon ao tonoranthorant\n",
            "Iteration 8001, Loss: 2.9386048316955566\n",
            "Generated text:arereane nour s t s indt heashoseaooor s thearao itha iso iit totis hes a t s t s t s a an hia t t is\n",
            "Iteration 9001, Loss: 3.003924608230591\n",
            "Generated text:ande wee st atheant\n",
            "\n",
            "Nu areeroonore, atoont w c s sar toue heaan an t s s a so aan s aa terat haa to \n"
          ]
        }
      ],
      "source": [
        "gpt_model = GPT(64, 4).to(device)\n",
        "# make you are running this on the GPU\n",
        "max_iters = 10000\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.SGD(gpt_model.parameters(), lr=learning_rate)\n",
        "for iter in range(max_iters):\n",
        "  optimizer.zero_grad()\n",
        "# sample a batch of data\n",
        "  xb, yb = get_batch()\n",
        "# evaluate the loss\n",
        "  logits, loss = gpt_model(xb, yb)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if iter%1000==0:\n",
        "      print(f'Iteration {iter+1}, Loss: {loss.item()}')\n",
        "      print(f'Generated text:{gpt_model.generate(start_char=\"a\", max_new_tokens=100,top_k=5,top_p=None, temperature=1.0)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy3v8Nv7YVUa"
      },
      "source": [
        "### Generate text\n",
        "\n",
        "\n",
        "print some text that your model generates"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gpt_model.generate(start_char=\"a\", max_new_tokens=100,top_k=5,top_p=None, temperature=1.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw9jbW-e6UiX",
        "outputId": "beea8e8c-5117-408a-e4f9-854fc9aceb44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ane serean ther s wn t wieriat, c tharan ina t ar isaande, isares hanind at w thon thare, hisa tothar\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}