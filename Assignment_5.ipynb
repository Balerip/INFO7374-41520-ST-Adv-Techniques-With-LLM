{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 5\n",
        "\n",
        "Agents are an emerging field thats use reflection, tools, planning, and multi agent collaboration\n",
        "\n",
        "\n",
        "In this assignment, we will build a research agent. We will use serverless LLM endpoints. To get started, you create an account with Together AI or Anthropic. They should provide you with a few dollars worth of credits that should be enough to complete the assignment. You are free to choose any other provider such as OpenAI, Mistral, Fireworks, OctoAI, or Groq. I encourage you to play around with different models to get a feel for how they work. For this assignment, the API usage cost should be around a couple dollars. Depending on the model you choose and how many attempts you use, it may be a couple cents. For OpenAI, Anthropic, and Mistral, double check what model you are using. The flagship models are significantly more expensive than the smaller models (pricing between models varies by 50x). For the purposes of this assignment, it is sufficient to use the smallest/cheapest models.\n",
        "TogetherAI, Fireworks, OctoAI, and Groq run open source models. For these, it’s better to run the mid-large tier models. Mixtral is a good place to start. It’s good to play around with different models.\n",
        "Many providers are able to use OpenAI's client library, but some do not (like Anthropic). Use whatever makes sense.\n",
        "You can run this on Colab with a CPU, or locally and submit the Jupyter notebook as your submission. Since we are using third party providers for the LLMs, we will not load the model locally. If you run on Colab, take special care to not leak your API key. Here’s an example of how to properly use secrets in Colab.\n",
        "\n",
        "Research Agent Build an LLM-based research agent that can take a research topic, find relevant information, and generate a short summary (~1 paragraph) on the given topic.\n",
        "Tools to Implement (20 points, 4 points each):\n",
        "Topic Breakdown Tool: Create a tool that takes a broad research topic and breaks it down into smaller, more focused subtopics or subqueries. You can use an LLM to generate these subtopics based on the main topic.\n",
        "Query Expansion Tool: Develop a tool to expand the subqueries generated by the Topic Breakdown Tool. The tool should generate related keywords, synonyms, and phrases to enhance the search results.\n",
        "Search Tool: Create a wrapper around the You API or Brave Search API. Please note that the free tier is 1000 queries/month. Consider creating a mock while developing, and switch to actually call the You API once the agent is more stable. Additionally, consider caching the search results.\n",
        "Critique Tool: Create a tool that critiques the summary, and offers suggestions of how to improve and potentially other relevant topics to search for.\n",
        "Summarizer Tool (optional): Create a tool that takes some input and summarizes its content using an LLM.\n",
        "\n",
        "Workflow (30 points) Implement an agent workflow that uses all of these tools. In the agent workflow, the agent should be provided with all the tools and it should decide which tool to use. For the individual tool implementations, if you use a call to an LLM you do not need to provide any tools.\n",
        "Sample Agent Workflow:\n",
        "The agent receives a research topic from the user.\n",
        "It uses the Topic Breakdown Tool to generate subtopics or subqueries.\n",
        "The Query Expansion Tool expands the subqueries with related keywords and phrases.\n",
        "The Search Tool uses the expanded queries and subqueries to gather relevant information from various sources.\n",
        "The agent generates the summary incorporating the search results. (optional)\n",
        "The agent critiques the summary, and improves the results. (optional)\n",
        "The agent presents the final summary to the user.\n",
        "The sample workflow is the minimum implementation requirement. Feel free to add more tools, add loops in the workflow, etc."
      ],
      "metadata": {
        "id": "H0f3X9zqQZjs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8jzmlAoDRf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f1a5657-e97e-4c3b-e38c-6da7b10d746b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: together in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /usr/local/lib/python3.10/dist-packages (from together) (3.9.5)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from together) (8.1.7)\n",
            "Requirement already satisfied: eval-type-backport<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from together) (0.1.3)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /usr/local/lib/python3.10/dist-packages (from together) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from together) (1.25.2)\n",
            "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /usr/local/lib/python3.10/dist-packages (from together) (10.3.0)\n",
            "Requirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from together) (14.0.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from together) (2.7.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from together) (2.31.0)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from together) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /usr/local/lib/python3.10/dist-packages (from together) (4.66.2)\n",
            "Requirement already satisfied: typer<0.13,>=0.9 in /usr/local/lib/python3.10/dist-packages (from together) (0.9.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (4.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->together) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->together) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->together) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->together) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzKWCWLYD8lm",
        "outputId": "9a7ff9a5-4bb3-4f45-eafb-02f82f6c9ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.23.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZEjHf39EE0R",
        "outputId": "cb66229b-c1a6-474e-8d28-bca17cbf07fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: openai\n",
            "Version: 1.23.2\n",
            "Summary: The official Python library for the openai API\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: OpenAI <support@openai.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: anyio, distro, httpx, pydantic, sniffio, tqdm, typing-extensions\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_w5OQQUDeoj"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca9XvN9yD7iN"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "my_secret_key = userdata.get('Together_api_token')\n",
        "my_secret_key_2=userdata.get('You_api_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEtdxbKnFpUV"
      },
      "outputs": [],
      "source": [
        "import together\n",
        "client = together.Client(api_key=my_secret_key,base_url='https://api.together.xyz/v1',)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs-H4H2AE0gX"
      },
      "outputs": [],
      "source": [
        "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnHw4I29Hyva"
      },
      "outputs": [],
      "source": [
        "def topic_breakdown_tool(research_topic):\n",
        "      chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "          {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an AI research assistant.\",\n",
        "          },\n",
        "          {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Generate 4 subQUERIES based on the research topic: {research_topic}.\",\n",
        "          }\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.9,\n",
        "        max_tokens=256,\n",
        "        top_k=40\n",
        "      )\n",
        "      subqueries=chat_completion.choices[0].message.content\n",
        "\n",
        "      return subqueries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAt0zNnyLxmM"
      },
      "outputs": [],
      "source": [
        "def query_expansion_tool(subqueries):\n",
        "    conversation = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an AI research assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Generate 4 keywords, synonyms, and phrases for each of the subqueries:{subqueries} and should follow the following format: {{'Subquery':Subquery,'Keywords':keywords,'Synonyms':Synonyms,'Phrases':phrases}}\"}\n",
        "    ]\n",
        "    # Request chat completions for generating keywords based on the conversation\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=conversation,\n",
        "        model=model_name,\n",
        "        temperature=0.9,\n",
        "        max_tokens=512,\n",
        "        top_k=40\n",
        "    )\n",
        "\n",
        "    # Extract keywords from the completion\n",
        "    expanded_queries = chat_completion.choices[0].message.content\n",
        "    return expanded_queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o43CUnHU8rNu"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from cachetools import TTLCache\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Initialize a cache with a time-to-live (TTL) of 3600 seconds (1 hour)\n",
        "\n",
        "\n",
        "def search_tool(query):\n",
        "    headers = {\"X-API-Key\": my_secret_key_2}\n",
        "    params = {\"query\": query}\n",
        "    response= requests.get(\n",
        "        f\"https://api.ydc-index.io/search?query={query}\",\n",
        "        params=params,\n",
        "        headers=headers,\n",
        "    ).json()\n",
        "    return response\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlKk0BlVey0w"
      },
      "outputs": [],
      "source": [
        "def summary_tool(search_result):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an AI research assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Provide Summary for the following search results generated:\\n\\n{search_result}.\"}\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages,\n",
        "        max_tokens=500,\n",
        "        stop=None,\n",
        "        temperature=0.8,\n",
        "    )\n",
        "    summary = response.choices[0].message.content\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8upeNbXtpaP4"
      },
      "outputs": [],
      "source": [
        "def critique_tool(summary):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an AI research assistant acting as a critique\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Please critique the following summary:\\n\\n{summary}. Recognize flaws within the summary, offer constructive criticism, and pinpoint areas for improvement\"}\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages,\n",
        "        max_tokens=500,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    critique = response.choices[0].message.content\n",
        "    return critique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXsm19VQqqP9"
      },
      "outputs": [],
      "source": [
        "def final_summary_based_on_critique(summary, critique):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an Ai assistant conducting research\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Enhance the provided summary: {summary} in accordance with the ensuing evaluation: {critique}.\"}\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages,\n",
        "        max_tokens=500,\n",
        "        stop=None,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    final_summary= response.choices[0].message.content\n",
        "    return final_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xycnO-wGudXf"
      },
      "outputs": [],
      "source": [
        "def research_agent():\n",
        "\n",
        "  topic_of_research = input(\"Welcome! I am your research assistant, ready to help you discover relevant topics and provide valuable summaries. Please enter your research topic::\\n\\n\\nTopic Of Research:\")\n",
        "  print(\"\\n\\n 1. Topic Breakdown Tool :\")\n",
        "  subqueries = topic_breakdown_tool(topic_of_research)\n",
        "  print(f\"\\n {subqueries}\")\n",
        "\n",
        "  print(\"\\n\\n 2. Query Expansion Tool :\")\n",
        "  extended_subqueries = query_expansion_tool(subqueries)\n",
        "  print(f\"\\n {extended_subqueries}\")\n",
        "\n",
        "  print(\"\\n\\n 3. Search Tool : \")\n",
        "  search_results = search_tool(extended_subqueries)\n",
        "  print(f\"\\n {search_results['hits']}\")\n",
        "\n",
        "  print(\"\\n\\n 4. Summary Tool :\")\n",
        "  summary = summary_tool(search_results['hits'])\n",
        "  print(f\"\\n {summary}\")\n",
        "\n",
        "  print(\"\\n\\n 5. Critique Tool :\")\n",
        "  critic_results = critique_tool(summary)\n",
        "  print(f\"\\n {critic_results}\")\n",
        "\n",
        "  print(\"\\n\\n 6. Final Summary Generator based on summary and crtique:\")\n",
        "  final_summary = final_summary_based_on_critique(summary, critic_results)\n",
        "  print(f\"\\n {final_summary}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSCWwXlK4A6O",
        "outputId": "52aeccf8-7245-420a-a042-6abbf0702ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome! I am your research assistant, ready to help you discover relevant topics and provide valuable summaries. Please enter your research topic::\n",
            "\n",
            "\n",
            "Topic Of Research:Advancements in LLM\n",
            "\n",
            "\n",
            " 1. Topic Breakdown Tool :\n",
            "\n",
            "  Sure, I'd be happy to help generate some subqueries for the research topic \"Advancements in Language Learning Models (LLM)\". Here they are:\n",
            "\n",
            "1. What are the latest architectural innovations in Language Learning Models, and how do they differ from previous models?\n",
            "2. How have recent advancements in Natural Language Processing (NLP) techniques impacted the development of Language Learning Models?\n",
            "3. What role do transfer learning and multitask learning play in the current advancements of Language Learning Models?\n",
            "4. How do the ethical considerations of Language Learning Models, such as bias and fairness, impact their advancement and implementation?\n",
            "\n",
            "These subqueries can help you explore different aspects of the overarching topic and provide a more comprehensive understanding of the advancements in LLM.\n",
            "\n",
            "\n",
            " 2. Query Expansion Tool :\n",
            "\n",
            "  Subquery 1:\n",
            "{'Subquery': 'Latest architectural innovations in Language Learning Models',\n",
            "'Keywords': ['latest architectural innovations', 'recent advancements', 'new developments'],\n",
            "'Synonyms': ['innovations', 'advancements', 'developments', 'breakthroughs'],\n",
            "'Phrases': ['state-of-the-art architectures in LLM', 'current trends in LLM architecture']}\n",
            "\n",
            "Subquery 2:\n",
            "{'Subquery': 'Impact of recent NLP techniques on Language Learning Models',\n",
            "'Keywords': ['impact', 'influence', 'effect', 'consequence'],\n",
            "'Synonyms': ['impact', 'influence', 'effect', 'consequence'],\n",
            "'Phrases': ['influence of NLP techniques on LLM development', 'impact of NLP advancements on LLM']}\n",
            "\n",
            "Subquery 3:\n",
            "{'Subquery': 'Role of transfer learning and multitask learning in Language Learning Models',\n",
            "'Keywords': ['role', 'function', 'contribution', 'importance'],\n",
            "'Synonyms': ['role', 'function', 'contribution', 'importance'],\n",
            "'Phrases': ['significance of transfer learning in LLM', 'function of multitask learning in LLM']}\n",
            "\n",
            "Subquery 4:\n",
            "{'Subquery': 'Ethical considerations of Language Learning Models, such as bias and fairness',\n",
            "'Keywords': ['ethical considerations', 'bias', 'fairness'],\n",
            "'Synonyms': ['ethical considerations', 'ethical concerns', 'fairness issues', 'bias problems'],\n",
            "'Phrases': ['addressing bias in LLM', 'promoting fairness in LLM development']}\n",
            "\n",
            "\n",
            " 3. Search Tool : \n",
            "\n",
            " [{'description': 'A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally ...', 'snippets': ['LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word. LLMs are artificial neural networks. The largest and most capable, as of March 2024, are built with a decoder-only transformer-based architecture while some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).', 'NLP researchers were evenly split when asked, in a 2022 survey, whether (untuned) LLMs \"could (ever) understand natural language in some nontrivial sense\". Proponents of \"LLM understanding\" believe that some LLM abilities, such as mathematical reasoning, imply an ability to \"understand\" certain concepts.', \"The matter of LLM's exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human like language. These aspects of language as a model of cognition have been developed in the field of cognitive linguistics.\", 'Advances in software and hardware have reduced the cost substantially since 2020, such that in 2023 training of a 12-billion-parameter LLM computational cost is 72,300 A100-GPU-hours, while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80 thousand and $1.6 million.', 'Some notable LLMs are OpenAI\\'s GPT series of models (e.g., GPT-3.5 and GPT-4, used in ChatGPT and Microsoft Copilot), Google\\'s PaLM and Gemini (the latter of which is currently used in the chatbot of the same name), xAI\\'s Grok, Meta\\'s LLaMA family of open-source models, Anthropic\\'s Claude models, Mistral AI\\'s open source models, and Databricks\\' open source DBRX. At the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\".'], 'title': 'Large language model - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Large_language_model'}, {'description': 'Discover the Top 9 Natural Language Processing Trends plus 18 Top Startups in the field to learn how they impact your business.', 'snippets': ['Based on the Natural Language Processing Innovation Map, the Tree Map below illustrates the impact of the Top 9 NLP Trends in 2023. Virtual assistants improve customer relationships and worker productivity through smarter assistance functions. Advances in learning models, such as reinforced and transfer learning, are reducing the time to train natural language processors.', 'The cost and resource-efficient development of NLP solutions is also a necessary requirement to increase their adoption. The Natural Language Processing Trends & Startups outlined in this report only scratch the surface of trends that we identified during our data-driven innovation & startup scouting process. Among others, transfer learning, semantic web, and behavior analysis will transform the sector as we know it today.', 'Moreover, the Natural Language Processing Innovation Map reveals 18 hand-picked startups, all working on emerging technologies that advance their field. ... Based on the Natural Language Processing Innovation Map, the Tree Map below illustrates the impact of the Top 9 NLP Trends in 2023.', 'Its deep learning-based NLP model perceives message context instead of focusing on keywords. Y Meadow’s semantics-based solution finds use across industries for customer issue handling. Spiky is a US startup that develops an AI-based analytics tool to improve sales calls, training, and coaching sessions.'], 'title': '9 Natural Language Processing Trends in 2023 | StartUs Insights', 'url': 'https://www.startus-insights.com/innovators-guide/natural-language-processing-trends/'}, {'description': 'Text-to-SQL is a task in natural language processing (NLP) that aims to automatically generate Structured Query Language (SQL) queries from natural language text. This task involves converting text…', 'snippets': ['TLDR: This article delves into the Text-to-SQL domain, demonstrating the growing reliance on Large Language Models (LLMs) for this complex…', 'This shows the LLM’s attempt to optimize query performance at a more advanced level, although not all such attempts are successful. Improvement: Using optimizer hints for join operations. Outcome: Success. Analysis: The LLM shifts strategy by using optimizer hints (USE_NL) to specifically instruct the database how to perform the joins. This shows a sophisticated understanding of database optimization techniques, aiming to make the query execution more efficient. The LLM learns from failures and successes.', 'This adds an element of unpredictability or ‘randomness’ to the model’s behavior, which is advantageous in generating more diverse and innovative responses. This approach is designed to optimize the feedback loop without the need for suffixing error codes. ... Enhances query efficiency and speed, optimizing for dynamic improvements. Suitable for high-performance settings where query execution time is a priority. ... Incurs higher costs due to multiple executions involving several calls to the LLM, increasing the expense per call.', 'Model Garden: Model Garden houses over 100 cutting-edge LLMs and task-specific models. This includes Google’s own first party models discussed above, and open-source models like Llama 2, TII’s Falcon, Stable Diffusion, Mistral and more. It streamlines the process of finding, deploying, and maintaining foundation models for developers via both API and console.'], 'title': 'Architectural Patterns for Text-to-SQL: Leveraging LLMs for Enhanced BigQuery Interactions | by Arun Shankar | Google Cloud - Community | Medium', 'url': 'https://medium.com/google-cloud/architectural-patterns-for-text-to-sql-leveraging-llms-for-enhanced-bigquery-interactions-59756a749e15'}, {'description': 'A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.', 'snippets': ['Large Language Models (LLMs) represent a breakthrough in artificial intelligence, employing neural network techniques with extensive parameters for advanced language processing. This article explores the evolution, architecture, applications, and challenges of LLMs, focusing on their impact in the field of Natural Language Processing (NLP). A large language model is a type of artificial intelligence algorithm that applies neural network techniques with lots of parameters to process and understand human languages or text using self-supervised learning techniques.', 'NLP is Natural Language Processing, a field of artificial intelligence (AI). It consists of the development of the algorithms. NLP is a broader field than LLM, which consists of algorithms and techniques. NLP rules two approaches i.e. Machine learning and the analyze language data. Applications of NLP are-', 'This article explores the evolution, architecture, applications, and challenges of LLMs, focusing on their impact in the field of Natural Language Processing (NLP). A large language model is a type of artificial intelligence algorithm that applies neural network techniques with lots of parameters to process and understand human languages or text using self-supervised learning techniques. Tasks like text generation, machine translation, summary writing, image generation from texts, machine coding, chat-bots, or Conversational AI are applications of the Large Languag.e Model. Examples of such LLM models are Chat GPT by open AI, BERT (Bidirectional Encoder Representations from Transformers) by Google, etc.', 'NLP rules two approaches i.e. Machine learning and the analyze language data. Applications of NLP are- ... Social Media Analytics. while on the other hand, LLM is a Large Language Model, and is more specific to human- like text, providing content generation, and personalized recommendations. Large Language Models (LLMs) come with several advantages that contribute to their widespread adoption and success in various applications: LLMs can perform zero-shot learning, meaning they can generalize to tasks for which they were not explicitly trained.'], 'title': 'What is a Large Language Model (LLM) - GeeksforGeeks', 'url': 'https://www.geeksforgeeks.org/large-language-model-llm/'}, {'description': 'Gain insights into Large Language Model (LLMs) and discover how they are revolutionizing language understanding in our comprehensive article. Explore Now!', 'snippets': ['It is important to approach LLMs with a critical eye and evaluate their impact on society. With careful use and continued development, LLMs have the potential to bring about positive changes in many domains, but we should be aware of their limitations and ethical implications. ... Large Language Models (LLMs) can understand complex sentences, understand relationships between entities and user intent, and generate new text that is coherent and grammatically correct · The article explores the architecture of some LLMs, including embedding, feedforward, recurrent, and attention layers.', 'LLMs can learn from big data, understand its context and entities, and answer user queries. This makes them a great alternative for regular usage in various tasks in several industries. However, there are concerns about the ethical implications and potential biases associated with these models. It is important to approach LLMs with a critical eye and evaluate their impact on society. With careful use and continued development, LLMs have the potential to bring about positive changes in many domains, but we should be aware of their limitations and ethical implications.', 'Large Language Model (LLMs) have revolutionized the field of natural language processing, allowing for new advancements in text generation and understanding. LLMs can learn from big data, understand its context and entities, and answer user queries. This makes them a great alternative for regular usage in various tasks in several industries. However, there are concerns about the ethical implications and potential biases associated with these models. It is important to approach LLMs with a critical eye and evaluate their impact on society.', 'Explore the future implications of LLMs, including their potential impact on job markets, communication, and society as a whole. This article was published as a part of the Data Science Blogathon. ... A large language model is an advanced type of language model that is trained using deep learning techniques on massive amounts of text data.'], 'title': 'What are Large Language Models(LLMs)?', 'url': 'https://www.analyticsvidhya.com/blog/2023/03/an-introduction-to-large-language-models-llms/'}, {'description': 'Large language models are driving AI hype in 2023. Learn which ones to follow.', 'snippets': [\"Falcon 40B is a transformer-based, causal decoder-only model developed by the Technology Innovation Institute. It is open source and was trained on English data. The model is available in two smaller variants as well: Falcon 1B and Falcon 7B (1 billion and 7 billion parameters). Amazon has made Falcon 40B available on Amazon SageMaker. It is also available for free on GitHub. Gemini is Google's family of LLMs that power the company's chatbot of the same name.\", \"Orca was developed by Microsoft and has 13 billion parameters, meaning it's small enough to run on a laptop. It aims to improve on advancements made by other open source models by imitating the reasoning procedures achieved by LLMs. Orca achieves the same performance as GPT-4 with significantly fewer parameters and is on par with GPT-3.5 for many tasks.\", \"Lamda (Language Model for Dialogue Applications) is a family of LLMs developed by Google Brain announced in 2021. Lamda used a decoder-only transformer language model and was pre-trained on a large corpus of text. In 2022, LaMDA gained widespread attention when then-Google engineer Blake Lemoine went public with claims that the program was sentient. It was built on the Seq2Seq architecture. Large Language Model Meta AI (Llama) is Meta's LLM released in 2023.\", \"Constant developments in the field can be difficult to keep track of. Here are some of the most influential models, both past and present. Included in it are models that paved the way for today's leaders as well as those that could have a significant effect in the future. ... Below are some of the most relevant large language models today. They do natural language processing and influence the architecture of future models. BERT is a family of LLMs that Google introduced in 2018.\"], 'title': '18 of the best large language models in 2024', 'url': 'https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models'}, {'description': 'Large language models are AI systems capable of understanding and generating human language by processing vast amounts of text data.', 'snippets': ['Outside of the enterprise context, it may seem like LLMs have arrived out of the blue along with new developments in generative AI. However, many companies, including IBM, have spent years implementing LLMs at different levels to enhance their natural language understanding (NLU) and natural language processing (NLP) capabilities. This has occurred alongside advances in machine learning, machine learning models, algorithms, neural networks and the transformer models that provide the architecture for these AI systems.', 'LLMs represent a significant breakthrough in NLP and artificial intelligence, and are easily accessible to the public through interfaces like Open AI’s Chat GPT-3 and GPT-4, which have garnered the support of Microsoft. Other examples include Meta’s Llama models and Google’s bidirectional encoder representations from transformers (BERT/RoBERTa) and PaLM models.', 'LLMs operate by leveraging deep learning techniques and vast amounts of textual data. These models are typically based on a transformer architecture, like the generative pre-trained transformer, which excels at handling sequential data like text input. LLMs consist of multiple layers of neural networks, each with parameters that can be fine-tuned during training, which are enhanced further by a numerous layer known as the attention mechanism, which dials in on specific parts of data sets.', 'These models are typically based on a transformer architecture, like the generative pre-trained transformer, which excels at handling sequential data like text input. LLMs consist of multiple layers of neural networks, each with parameters that can be fine-tuned during training, which are enhanced further by a numerous layer known as the attention mechanism, which dials in on specific parts of data sets. During the training process, these models learn to predict the next word in a sentence based on the context provided by the preceding words.'], 'title': 'What Are Large Language Models (LLMs)? | IBM', 'url': 'https://www.ibm.com/topics/large-language-models'}, {'description': 'A reference architecture for the LLM app stack. It shows the most common systems, tools, and design patterns used by AI startups and tech companies.', 'snippets': ['In this post, we’re sharing a reference architecture for the emerging LLM app stack. It shows the most common systems, tools, and design patterns we’ve seen used by AI startups and sophisticated tech companies. This stack is still very early and may change substantially as the underlying technology advances, but we hope it will be a useful reference for developers working with LLMs now.', 'There are many different ways to build with LLMs, including training models from scratch, fine-tuning open-source models, or using hosted APIs. The stack we’re showing here is based on in-context learning, which is the design pattern we’ve seen the majority of developers start with (and is only possible now with foundation models).', 'Most developers we spoke with haven’t gone deep on operational tooling for LLMs yet. Caching is relatively common—usually based on Redis—because it improves application response times and cost. Tools like Weights & Biases and MLflow (ported from traditional machine learning) or PromptLayer and Helicone (purpose-built for LLMs) are also fairly widely used.', 'Tools like Weights & Biases and MLflow (ported from traditional machine learning) or PromptLayer and Helicone (purpose-built for LLMs) are also fairly widely used. They can log, track, and evaluate LLM outputs, usually for the purpose of improving prompt construction, tuning pipelines, or selecting models. There are also a number of new tools being developed to validate LLM outputs (e.g., Guardrails) or detect prompt injection attacks (e.g., Rebuff).'], 'title': 'Emerging Architectures for LLM Applications | Andreessen Horowitz', 'url': 'https://a16z.com/emerging-architectures-for-llm-applications/'}, {'description': 'Learn how the AI algorithm known as a large language model, or LLM, uses deep learning and large data sets to understand and generate new content.', 'snippets': [\"One example of a language representation model is Google's Bert, which makes use of deep learning and transformers well suited for NLP. Multimodal model. Originally LLMs were specifically tuned just for text, but with the multimodal approach it is possible to handle both text and images. GPT-4 is an example of this type of model. Generative AI challenges that businesses should consider ... The future of LLMs is still being written by the humans who are developing the technology, though there could be a future in which the LLMs write themselves, too.\", 'This is a diagram of the architecture for a transformer model. LLMs have become increasingly popular because they have broad applicability for a range of NLP tasks, including the following:', \"Additional training on top of a zero-shot model such as GPT-3 can lead to a fine-tuned, domain-specific model. One example is OpenAI Codex, a domain-specific LLM for programming based on GPT-3. Language representation model. One example of a language representation model is Google's Bert, which makes use of deep learning and transformers well suited for NLP.\", 'Enabling more accurate information through domain-specific LLMs developed for individual industries or functions is another possible direction for the future of large language models. Expanded use of techniques such as reinforcement learning from human feedback, which OpenAI uses to train ChatGPT, could help improve the accuracy of LLMs too.'], 'title': 'What are Large Language Models? | Definition from TechTarget', 'url': 'https://www.techtarget.com/whatis/definition/large-language-model-LLM'}, {'description': 'Define a large language model, understand how it works, its benefits, and challenges, and explore examples of large language models....', 'snippets': ['Stay up to date with the latest tech topics, innovations, and news. ... Grow your skills and open doors for future success. ... Find the support you need, no matter the topic. ... A large language model (LLM) is a deep learning algorithm that can perform a variety of natural language processing (NLP) tasks.', \"To address the current limitations of LLMs, the Elasticsearch Relevance Engine (ESRE) is a relevance engine built for artificial intelligence-powered search applications. With ESRE, developers are empowered to build their own semantic search application, utilize their own transformer models, and combine NLP and generative AI to enhance their customers' search experience.\", \"With ESRE, developers are empowered to build their own semantic search application, utilize their own transformer models, and combine NLP and generative AI to enhance their customers' search experience. Supercharge your relevance with the Elasticsearch Relevance Engine · Choosing an LLM: The 2024 getting started guide to open-source LLMs\", \"As large language models continue to grow and improve their command of natural language, there is much concern regarding what their advancement would do to the job market. It's clear that large language models will develop the ability to replace workers in certain fields. In the right hands, large language models have the ability to increase productivity and process efficiency, but this has posed ethical questions for its use in human society. ... To address the current limitations of LLMs, the Elasticsearch Relevance Engine (ESRE) is a relevance engine built for artificial intelligence-powered search applications.\"], 'title': 'What is a Large Language Model? | A Comprehensive LLMs Guide | Elastic', 'url': 'https://www.elastic.co/what-is/large-language-models'}]\n",
            "\n",
            "\n",
            " 4. Summary Tool :\n",
            "\n",
            "  The search results provide information on Large Language Models (LLMs) from various sources. Here is a summary of the key points:\n",
            "\n",
            "1. Large Language Models (LLMs) are language models capable of achieving general-purpose language generation and other natural language processing tasks. They acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation and are artificial neural networks, with the most capable models built with a decoder-only transformer-based architecture.\n",
            "2. According to a 2022 survey, NLP researchers were evenly split on whether untuned LLMs could understand natural language in a nontrivial sense. The debate revolves around how to model thought and language in a computer system and how to enable the computer system to generate human-like language.\n",
            "3. Advances in software and hardware have reduced the cost of training LLMs, with the computational cost of training a 12-billion-parameter LLM being 72,300 A100-GPU-hours in 2023.\n",
            "4. Notable LLMs include OpenAI's GPT series, Google's PaLM and Gemini, xAI's Grok, Meta's LLaMA family, Anthropic's Claude models, Mistral AI's open-source models, and Databricks' open-source DBRX.\n",
            "5. Natural Language Processing (NLP) Trends in 2023 include the use of virtual assistants, advances in learning models, cost and resource-efficient development of NLP solutions, transfer learning, semantic web, and behavior analysis.\n",
            "6. LLMs can understand message context instead of focusing on keywords, and can be used for semantics-based solutions across industries for customer issue handling.\n",
            "7. Text-to-SQL is a task in NLP that aims to automatically generate SQL queries from natural language text. LLMs can optimize query performance at a more advanced level, but may not always be successful.\n",
            "8. LLMs can perform zero-shot learning, meaning they can generalize to tasks for which they were not explicitly trained.\n",
            "9. There are ethical implications and potential biases associated with LLMs, and careful use and continued development are\n",
            "\n",
            "\n",
            " 5. Critique Tool :\n",
            "\n",
            "  The summary you provided is informative and covers a wide range of topics related to Large Language Models (LLMs). However, there are some areas where it could be improved:\n",
            "\n",
            "1. **Clarity and Flow:** While the summary is generally well-organized, some transitions between ideas could be smoother. For example, the connection between the discussion of LLMs in general and the specific trends in NLP for 2023 could be made more explicit.\n",
            "\n",
            "2. **Supporting Evidence:** Some of the claims in the summary, such as the specific computational cost of training a 12-billion-parameter LLM, would benefit from a citation to the original source. This would strengthen the credibility of the information presented.\n",
            "\n",
            "3. **Depth:** The summary provides a good overview of LLMs, but some topics could be explored in more depth. For instance, the discussion of ethical implications and potential biases associated with LLMs could be expanded to include specific examples and how these issues are being addressed.\n",
            "\n",
            "4. **Accuracy:** The summary states that LLMs can understand message context instead of focusing on keywords. While it's true that LLMs can consider context, it's important to note that they still primarily operate based on statistical patterns in the data they were trained on, and may not truly \"understand\" context in the way humans do.\n",
            "\n",
            "5. **Balance:** The summary could benefit from a more balanced view of LLMs. While they have many potential applications and benefits, it's also important to acknowledge their limitations and the challenges associated with their use.\n",
            "\n",
            "6. **Consistency:** The summary mentions both \"natural language processing tasks\" and \"natural language understanding\" in relation to LLMs. While these terms are related, they are not identical, and the summary could be clearer about what specific capabilities LLMs have.\n",
            "\n",
            "7. **Up-to-date Information:** The survey about NLP researchers' views on LLMs is from 2022. It would be helpful to provide more recent information if it's available.\n",
            "\n",
            "Overall, the summary is a good starting point, but could be improved with more careful attention to clarity, evidence, depth, balance, consistency, and recency.\n",
            "\n",
            "\n",
            " 6. Final Summary Generator based on summary and crtique:\n",
            "\n",
            "  Large Language Models (LLMs) are a type of artificial neural network that can achieve general-purpose language generation and other natural language processing (NLP) tasks. They acquire these abilities through a computationally intensive self-supervised and semi-supervised training process on text documents. The most capable LLMs typically use a decoder-only transformer-based architecture for text generation (Vaswani et al., 2017).\n",
            "\n",
            "As of 2023, there is ongoing debate among NLP researchers about whether untuned LLMs can understand natural language in a nontrivial sense. Some researchers argue that LLMs can model thought and language in a computer system, enabling them to generate human-like language. Others, however, are skeptical about the depth of this understanding (Schakel & Shutova, 2022).\n",
            "\n",
            "Recent advances in software and hardware have reduced the cost of training LLMs. For example, in 2023, the computational cost of training a 12-billion-parameter LLM was 72,300 A100-GPU-hours (OpenAI, 2023). Notable LLMs include OpenAI's GPT series, Google's PaLM and Gemini, xAI's Grok, Meta's LLaMA family, Anthropic's Claude models, Mistral AI's open-source models, and Databricks' open-source DBRX.\n",
            "\n",
            "In 2023, several trends are shaping the field of NLP, including the use of virtual assistants, advances in learning models, cost and resource-efficient development of NLP solutions, transfer learning, semantic web, and behavior analysis. LLMs can be used for semantics-based solutions across industries for customer issue handling, understanding message context instead of focusing solely on keywords (Khandelwal et al., 2020).\n",
            "\n",
            "One task in NLP that LLMs can optimize is text-to-SQL, which involves generating SQL queries from natural language text. LLMs can improve query performance at an advanced level, but they may not always be successful (Suhr et al., 2020). Additionally, LLMs can perform zero-shot learning, meaning they can generalize to tasks for which they\n"
          ]
        }
      ],
      "source": [
        "research_agent()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}